---
title: "So you want to backpropagate"
author: "LoÃ¯c Grobol"
date: "2024-01-24"
categories: [maths, neural networks, machine learning, optimisation, algorithm]
image: "image.jpg"
image-alt: "Image of rat primary cortical neurons in culture."
bibliography: biblio.bib
---

![](image.jpg){fig-alt="Image of rat primary cortical neurons in culture"}

*Backpropagation made right for a certain value of right.*

<small>*Header by <a href="https://flickr.com/photos/zeissmicro/30614937102/">ZEISS Microscopy</a>, <a href="https://creativecommons.org/licenses/by/2.0/">CC BY 2.0</a>, via Flickr*</small>

I have just taught the SGD algorithm with backpropagation to my NLP masters student and, as every year, I found that no explanation of it on the web looks satisfying to me. As the saying goes â€œ*on est jamais mieux serviâ‹…e que par soi-mÃªme*â€, so I figured I'd better get one. In the worst case, it'll be useful for a future version of me (hi mate!).

Backpropagation is a very frustrating topic. At its core, it's nothing very complicated and you never need more than relatively basic Bachelor-level maths (a bit of linear algebra, a bit of multivariate calculus). On the other hand it does involve the manipulation of many objects simultaneously, making it hard to keep everything in mind simultaneously, and even to write down the process, due to the sheer number of quantities involved. I am doing the best I can, but I realise that it can be easy to get bogged down. I can only offer a few words of advice:

- We will split the problem in several smaller problems. Only focus on one of them at a time.
- Trust the process.
- When you have fully understood all the small parts, only then put them back together.

I **hate** doing things this way and I am not good at it at all, but in my experience, it's the best way to get around this little frustating piece of knowledge.

## Problem formulation

We want to optimise a neural network, i.e. to find the weights that minimize the loss on a train dataset. Formally, consider the error function:

$$
E(Î¸) = \sum_{(X, y)âˆˆ\mathcal{D}}\operatorname{loss}(\operatorname{net}_Î¸(X), y)
$$

where $\operatorname{net}_Î¸$ is the function implemented by the neural network when its weights are $Î¸$, $\operatorname{loss}$ is the chosen per-sample loss and $\mathcal{D}=\{(X_1, y_1), â€¦, (X_n, y_n)\}$ is the training dataset.

$E$ measures the overall error made by the network on the training dataset for a given parameter vector $Î¸$. In this context â€œtrainingâ€ the network means finding a value for $Î¸$ such that $E(Î¸)$ is minimal.

There are many algorithms for that, but for neural networks, most of the time, an efficient (possibly approximate) solution to this problem is to use a gradient-based algorithm, most commonly a variant of the Stochastic Gradient Descent algorithm (SGD). These algorithms have (obviously) one thing in common: they require to compute the gradient of the per-sample losses with respect to $Î¸$. In other words, for all $(X, y)âˆˆ\mathcal{D}$, we need to be able to compute the gradient of the per-sample error function

$$
âˆ‡_{\!Î¸}~e_{(X, y)}(Î¸) =  âˆ‡_Î¸~\operatorname{loss}(\operatorname{net}_Î¸(X), y)
$$

## Notations

> We are embarking on a journey with a lot of variables. Absurdly many variables. I'm trying my
> best to use as few as possible and to keep the notations as clear as I can but I'm still not very
> satisfied about it. If you can think of improvements, please let me knowâ€¯!

Let's assume that we have a neural network with $N$ fully connected layers and using a non-linearity $Ï†$. GivenÂ an input $X$, that network will compute an output $\hat{y}$ as:

$$
\hat{y} = f_N(f_{N-1}(â€¦ f_1(X)â€¦))
$$

Or equivalently

$$
\hat{y} = (f_Nâˆ˜f_{N-1}âˆ˜â€¦âˆ˜f_1)(X)
$$

Where $f_â„“$ is a *fully connected neural layer* (hence $â„“$): a function of the form

$$
\left\lvert
	\begin{array}{rrl}
		f_â„“:~& â„^{d_{â„“-1}} \longrightarrow & â„^{d_â„“}\\
			 & U \longmapsto & f_â„“(U) = Ï†(Z) \stackrel{\mathrm{def}}{=} \begin{pmatrix}Ï†(z_1)\\â‹®\\Ï†(z_r)\end{pmatrix}
	\end{array}
\right.
$$

Where

$$
Z = W^â„“Ã—U
$$

$W^â„“âˆˆ\mathcal{Mat}(â„^{d_â„“}, â„^{d_{â„“-1}})$ being the weight matrix of $â„“$-th layer. We will also use the following notation for the weights of $W^â„“$

$$
W^i =
  \begin{pmatrix}
	w^â„“_{1,1} & â€¦ & w^â„“_{1, d_{â„“-1}}\\
	â‹®        &   & â‹®\\
	 w^â„“_{d_â„“,1} & â€¦ & w^â„“_{d_â„“, d_{â„“-1}}\\
  \end{pmatrix}
$$

> In theory, of course, $W^â„“$ could also be the $â„“$-th power of $W$, so the notation I use here is
> ambiguous. On the other hand, we do need to put that indice somewhere and it's a convenient place.
> In any case, we won't use any power of anything in this section, so let's agree that it's ok.

> Note that in general, neural layers are biasedâ€¯: of the form $f(U) = Ï†(WÃ—U + b)$. We will see
> later why this is makes no difference. For now, let's assume that our layers have no bias, it will
> makes our computations easier to follow.

So to sum it up, our neural network is defined by

- The sequence $(d_0, d_1, â€¦, d_N)$ of the dimensions of its layers. $d_0$ is the dimension of the input, and for all $â„“â©¾0$, $d_â„“$ is the dimension of the $â„“$-th layer.
- The weight matrices $(W^1, â€¦, W^N)$, where $W^â„“$ is of dimension $d_â„“$ rows by $d_{â„“-1}$ columns.
- The non-linearity $Ï†$. We will assume that it's used for every layer except the last, i.e. $f_N(U) = W^NÃ—U$. It will simplify things a bit and it's consistent with the general practice of using a specific non-linearity tied to the loss for the last layer (usually a softmax, that is mean to be used with the negative log-likelihood loss).

And we have:

$$
\hat{y} =
	W^N Ã— \underbrace{Ï†(
		\underbrace{W^{N-1} Ã—
			Ï†(
				â€¦
				Ã— \underbrace{Ï†(\underbrace{W^1Ã—X}_{Z^1})}_{O^1}â€¦
			)
		}_{Z^{N-1}}
	)}_{O^{N-1}}
$$

I have added yet a few more notations here:

- $Z^â„“=\begin{pmatrix}z^â„“_1\\â‹®\\z^â„“_{d_â„“}\end{pmatrix}$ is the intermediary output of the $â„“$-th layer, just before applying 
  the non-linearity.
- $O^â„“=\begin{pmatrix}o^â„“_1\\â‹®\\o^â„“_{d_â„“}\end{pmatrix}$ is the output of the $â„“$-th layer (including the non-linearity).

In other words, using a recursive definition:

$$
\left\lbrace
\begin{aligned}
	O^0 &= X\\
	Z^â„“ &= W^{â„“}Ã—O^{â„“-1} & \text{for $1 â©½ â„“ â©½ N-1$}\\
	O^â„“ &= Ï†(Z^â„“) & \text{for $1 â©½ â„“ â©½ N-1$}\\
	\hat{y} &= Z^N\\
\end{aligned}
\right.
$$ {#eq-def-z-o}

And that's it! **Now** we have all we need.

## Backpropagation proper

Remember: our problem here is to compte the gradient of the per-sample error function with respect to the parameters of the network.

$$
âˆ‡_{\!Î¸}~e_{(X, y)}(Î¸) =  âˆ‡_Î¸~\operatorname{loss}(\operatorname{net}_Î¸(X), y)
$$

With our notations, the parameters of the networks are the elements of the weight matrices. Therefore:

$$
Î¸ = (w^1_{1,1}, w^1_{1, 2}, â€¦, w^â„“_{i, j}, â€¦, w^N_{d_N, d_{N-1}})
$$

So, using the definition of the gradient, we have for all $(X, y)$,

$$
âˆ‡_{\!Î¸}~e_{(X, y)}(Î¸) =
	\begin{pmatrix}
		\frac{âˆ‚e_{(X, y)}}{âˆ‚w^1_{1,1}}\\
		â‹®\\
		\frac{âˆ‚e_{(X, y)}}{âˆ‚w^â„“_{i,j}}\\
		â‹®\\
		\frac{âˆ‚e_{(X, y)}}{âˆ‚w^N_{d_N, d_{N-1}}}\\
	\end{pmatrix}
$$

In order to make it easier on the eyes, we'll simplify the notation a bit and write that

$$
âˆ‡\,e(Î¸) = \left(\frac{âˆ‚e}{âˆ‚w^â„“_{i,j}}\right)_{â„“, i, j}
$$

since none of these symbols should be ambiguous.

Great news! That means that in order to arrive to our ends (train a neural network, remember), all we need is to be able to compute the value of $\frac{âˆ‚e}{âˆ‚w^â„“_{i,j}}$ for any appropriate $â„“$, $i$ and $j$.

In general this is a hard problem, both in terms of human-powered symbolic derivation and of computer-power numeric computations. The main reason is that while the influence of one weight on the output of its layer is fairly simple, its influence on the output of the following layers gets gradually more complicated.

Fortunately, we have one great tool â€” the chain rule â€” and one angle of attack â€” there is no backward connexion in neural networks.

More concretely, from the notations defined in @eq-def-z-o:

$$
\left\lbrace
\begin{aligned}
	e &= f_N(f_{N-1}(â€¦ f_{â„“+1}(O^â„“)â€¦))\\
	Z^â„“ &= W^â„“Ã—O^{â„“-1}\\
	O^â„“ &= Ï†(Z^â„“)\\
\end{aligned}
\right.
$$ {#eq-link-o-z-w}

Therefore, using the chain rule[^chain_rule]:

$$
\begin{align}
\frac{âˆ‚e}{âˆ‚w^â„“_{i,j}} 
	&=
		\left\langle
			âˆ‡_{\!O^â„“}\,e
		\middle|
			\frac{âˆ‚O^â„“}{âˆ‚w^â„“_{i,j}}
		\right\rangle\\
	&\stackrel{\mathrm{def}}{=}
		\left\langle
			âˆ‡_{\!O^â„“}\,e
		\middle|
			\begin{pmatrix}
				\frac{âˆ‚o^â„“_1}{âˆ‚w^â„“_{i,j}}\\
				â‹®\\
				\frac{âˆ‚o^â„“_{d_â„“}}{âˆ‚w^â„“_{i,j}}\\
			\end{pmatrix}
		\right\rangle\\
	&= \sum_k \frac{âˆ‚e}{âˆ‚o^â„“_k}\frac{âˆ‚o^â„“_k}{âˆ‚w^â„“_{i,j}}\\
\end{align}
$$ {#eq-de-dw-def}

where $âŸ¨â‹…|â‹…âŸ©$ is the scalar product.

[^chain_rule]: To be precise: the version of the chaine rule for $h=gâˆ˜f$, where $f: â„\longrightarrow â„^n$ and $g: â„^n\longrightarrow â„$. In that case:
$$
h'(t) =
	\left\langle
		âˆ‡_{\!f(t)}\,g(f(t))~
	\middle|~
		f'(t)
	\right\rangle
$$
Which is itself simply a special case of the chain rule for vector-valued functions with vector inputs:
$$
J_v(gâˆ˜f) = J_{f(v)}(g) Ã— J_v(f)
$$
where $J$ is the Jacobian operator.

But we know from @eq-link-o-z-w that for all $k$, $o^â„“_k=Ï†(z^â„“_k)$, so, using the chain rule,

$$
\begin{align}
	\frac{âˆ‚o^â„“_k}{âˆ‚w^â„“_{i,j}}
		&= \frac{âˆ‚Ï†(z^â„“_k)}{âˆ‚w^â„“_{i,j}}\\
		&= \frac{âˆ‚Ï†(z^â„“_k)}{âˆ‚z^â„“_k}\frac{âˆ‚z^â„“_k}{âˆ‚w^â„“_{i,j}}\\
		&= Ï†'(z^â„“_k)\frac{âˆ‚z^â„“_k}{âˆ‚w^â„“_{i,j}}
\end{align}
$$ {#eq-do-dw}



Moreover, using @eq-link-o-z-w and the definition of the matrix-vector product,

$$
z^â„“_k = \sum_m w^â„“_{k,m}o^{â„“-1}_m
$$

And therefore we have

$$
\begin{aligned}
	\frac{âˆ‚z^â„“_k}{âˆ‚w^â„“_{i,j}}
		&= \frac{âˆ‚}{âˆ‚w^â„“_{i,j}} \left(\sum_m w^â„“_{k,m}o^{â„“-1}_m\\\right)\\
		&= \sum_m \frac{âˆ‚}{âˆ‚w^â„“_{i,j}}(w^â„“_{k,m}o^{â„“-1}_m)
\end{aligned}
$$ {#eq-dz-dw-sum}

**But** in our neural network, there is no back-connection: the weights of a layer don't have any influence on the output of the previous layer. Therefore $o^{â„“-1}_m$ is does not depend on any $w^â„“_{k,m}$ and

$$
\frac{âˆ‚}{âˆ‚w^â„“_{i,j}}(w^â„“_{k,m}o^{â„“-1}_m) = 
	\begin{dcases*}
		o^{â„“-1}_j & if $(k, m) = (i, j)$\\
		0 & otherwise
	\end{dcases*}
$$ {#eq-dwo-dw}

Plugging @eq-dwo-dw into @eq-dz-dw-sum, since the terms of the sum where $mâ‰ j$ are all $0$ we get:

$$
\frac{âˆ‚z^â„“_k}{âˆ‚w^â„“_{i,j}} =
	\begin{dcases*}
		o^{â„“-1}_j & if $k = i$\\
		0 & otherwise
	\end{dcases*}
$$ {#eq-dz-dw}

Similarly, if we plug @eq-do-dw, then @eq-dz-dw into @eq-de-dw-def, since the terms of the sum where $kâ‰ i$ are all $0$ we get:

$$
\begin{align}
	\frac{âˆ‚e}{âˆ‚w^â„“_{i,j}}
		&= \sum_k \frac{âˆ‚e}{âˆ‚o^â„“_k}\frac{âˆ‚o^â„“_k}{âˆ‚w^â„“_{i,j}}\\
		&= \sum_k \frac{âˆ‚e}{âˆ‚o^â„“_k}\,Ï†'(z^â„“_k)\,\frac{âˆ‚z^â„“_k}{âˆ‚w^â„“_{i,j}}\\
		&= \frac{âˆ‚e}{âˆ‚o^â„“_i}\,Ï†'(z^â„“_i)\,o^{â„“-1}_j\\
\end{align}
$$

All that's left to compute is $\frac{âˆ‚e}{âˆ‚o^â„“_i}$. Let's take a samll step back and consider what that quantity is:

- $o^â„“_i$ is the $i$-th coordinate of $O^â„“$: the output of the $â„“$-th layer.
- $e$ is the error.
- $\frac{âˆ‚e}{âˆ‚o^â„“_i}$ is the $i$-th coordinate of $âˆ‡_{\!O^â„“}\,e$: the gradient of the error with respect to the output of the $â„“$-th layer.

There is a special case here for $â„“=N$: $O^N$ is $\hat{y}$, the output of our network and we have $e=\operatorname{loss}(\hat{y}, y)$, so:

$$
\begin{aligned}
	\frac{âˆ‚e}{âˆ‚o^N_i}
		&= \frac{âˆ‚}{âˆ‚o^N_i}\operatorname{loss}(\hat{y}, y)\\
		&=  \frac{âˆ‚}{âˆ‚\hat{y}_i}\operatorname{loss}(\hat{y}, y)
\end{aligned}
$$

And that's usually easy enough to compute[^nll-loss-deriv], since the derivatives of the loss function are usually well-known.

[^nll-loss-deriv]: For instance if the loss is the negative $\log$-likelihood applied on a softmax and $y=c$, with $1 â©½ c â©½ d_N$:
$$
\begin{align}
	\frac{âˆ‚}{âˆ‚\hat{y}_i}\operatorname{loss}(\hat{y}, y)
		& = \frac{âˆ‚}{âˆ‚\hat{y}_i}\Big(-\hat{y}_c + \operatorname{log sum exp}(\hat{y})\Big)\\
		&= 
			\begin{dcases*}
				-1 + \frac{\operatorname{e}^{\hat{y}_c}}{\sum_j \operatorname{e}^{\hat{y}_j}} & if $i=c$\\
				\frac{\operatorname{e}^{\hat{y}_i}}{\sum_j \operatorname{e}^{\hat{y}_j}} & otherwise
			\end{dcases*}\\
		&= \operatorname{softmax}(\hat{y})_i -ğŸ™_{i=c}
\end{align}
$$
For numerical stability reasons, this is usually evaluated by first computing the $\operatorname{log softmax}$ of $y$, from which it is easy to compute both the $\log$-likelihood and the $\operatorname{softmax}$ that appears in the last line of this derivation.
